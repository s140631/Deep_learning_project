{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import torch.distributions as tdist\n",
    "import sys\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d, BatchNorm2d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n",
    "from scipy.stats import norm\n",
    "# matplotlib options\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the pre-processed data\n",
    "df = pd.read_csv('prepared/demand_supply/10244.csv')\n",
    "#Baseline\n",
    "df.time = pd.to_datetime(df.time)\n",
    "demand_ts = [df.demand[t] for t in range(len(df))]\n",
    "supply_ts = [df.supply[t] for t in range(len(df))]\n",
    "print(\"Fraction of boundary cases: %.3f\" % (np.sum(np.array(demand_ts) == np.array(supply_ts)) / len(demand_ts)),)\n",
    "print(\"Fraction of problematic cases: %.3f\" % (np.sum(np.array(demand_ts) > np.array(supply_ts)) / len(demand_ts)),)\n",
    "plt.plot(demand_ts[:(24*7)], \"r-\")\n",
    "plt.plot(supply_ts[:(24*7)], \"b-\")\n",
    "plt.legend([\"demand\", \"supply\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "\n",
    "plot_acf(df.demand, lags=48)\n",
    "plot_pacf(df.demand, lags=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add more data from other zones:\n",
    "\n",
    "df2 = pd.read_csv('prepared/demand_supply/10222.csv')\n",
    "df2.time = pd.to_datetime(df.time)\n",
    "\n",
    "df3 = pd.read_csv('prepared/demand_supply/10234.csv')\n",
    "df3.time = pd.to_datetime(df.time)\n",
    "\n",
    "df4 = pd.read_csv('prepared/demand_supply/17304.csv')\n",
    "df4.time = pd.to_datetime(df.time)\n",
    "\n",
    "df = pd.concat([df,df2,df3,df4], axis = 0).reset_index(drop=True)\n",
    "df.time = pd.to_datetime(df.time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    if df.demand[i] > df.supply[i]:\n",
    "        df.supply[i] = df.demand[i]\n",
    "        \n",
    "dim = df.shape[0]\n",
    "\n",
    "ToD = [df.time[i].hour for i in range(dim)]\n",
    "DoW = [df.time[i].weekday() for i in range(dim)]\n",
    "DoM = [df.time[i].day for i in range(dim)]\n",
    "Month = [df.time[i].month for i in range(dim)]\n",
    "df['ToD'] = ToD\n",
    "df['DoW'] = DoW\n",
    "df['DoM'] = DoM\n",
    "df['Month'] = Month\n",
    "\n",
    "df = df[['Month','DoM','DoW','ToD','demand','supply']].sort_values(['Month', 'DoM','DoW','ToD']).reset_index(drop=True)\n",
    "Days = ['Mon', 'Tue','Wed','Thu','Fri','Sat','Sun']\n",
    "#df.DoW = [Days[each] for each in df.DoW]\n",
    "#Converting to categorical - the month and date is excluded for now\n",
    "df = pd.concat([df, pd.get_dummies(df.DoW), pd.get_dummies(df.ToD)], axis = 1)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,6:]\n",
    "y = df.demand\n",
    "z = df.supply\n",
    "\n",
    "#Prepare data for training\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "z = np.asarray(z)\n",
    "\n",
    "  \n",
    "X_lag = np.transpose(np.asmatrix([np.roll(y,1), np.roll(y,2), np.roll(y,3), np.roll(y,4), np.roll(y,5), np.roll(y,6), np.roll(y,7), np.roll(y,8), np.roll(y,9), np.roll(y,10), np.roll(y,11), np.roll(y,12), np.roll(y,13), np.roll(y,14), np.roll(y,15), np.roll(y,16), np.roll(y,17), np.roll(y,18), np.roll(y,19), np.roll(y,20), np.roll(y,21), np.roll(y,22), np.roll(y,23), np.roll(y,24)]))\n",
    "X_lag = np.expand_dims(X_lag, axis = 2)\n",
    "\n",
    "obs, time_steps, lag_dim = X_lag.shape\n",
    "num_features = X.shape[1]\n",
    "\n",
    "train_idx, val_idx = int(obs*0.8), int(obs*0.9)\n",
    "\n",
    "x_train = X[:train_idx,:].astype('float32')\n",
    "x_train_lag = X_lag[:train_idx,:].astype('float32')\n",
    "targets_train = y[:train_idx].astype('float32')\n",
    "supply_train = z[:train_idx].astype('float32')\n",
    "\n",
    "x_valid = X[train_idx:val_idx,:].astype('float32')\n",
    "x_valid_lag = X_lag[train_idx:val_idx,:].astype('float32')\n",
    "targets_valid = y[train_idx:val_idx].astype('float32')\n",
    "supply_valid = z[train_idx:val_idx].astype('float32')\n",
    "\n",
    "x_test = X[val_idx:,:].astype('float32')\n",
    "x_test_lag = X_lag[val_idx:,:].astype('float32')\n",
    "targets_test = y[val_idx:].astype('float32')\n",
    "supply_test = z[val_idx:].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "batch_size = 128\n",
    "hidden_rnn = 100\n",
    "hidden_FF = 126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, hidden_rnn):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = 1\n",
    "        self.hidden_rnn = hidden_rnn\n",
    "        \n",
    "        self.rnn = nn.LSTM(lag_dim, hidden_rnn, self.num_layers, batch_first = True)\n",
    "        self.rnn_dense = nn.Linear(in_features = self.hidden_rnn, out_features = self.hidden_rnn)\n",
    "        self.rnn_dense2 = nn.Linear(in_features = self.hidden_rnn, out_features = 1)\n",
    "        \n",
    "        \n",
    "        self.dropout = Dropout(0.3)\n",
    "      \n",
    "        \n",
    "        \n",
    "    def forward(self, time_series, h0, c0):\n",
    "        # Input shape [batch, seq_in_len, num_layers]\n",
    "        \n",
    "        output_rnn, (hn, cn) = self.rnn(time_series, (h0, c0))\n",
    "        out_rnn = self.dropout(self.rnn_dense(hn[-1]))\n",
    "        out_rnn = self.rnn_dense2(out_rnn)     \n",
    "        \n",
    "        return out_rnn\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        init = Variable(torch.randn(self.num_layers, batch_size, self.hidden_rnn)).float()\n",
    "        return init\n",
    "    def init_cell(self, batch_size):\n",
    "        init = Variable(torch.randn(self.num_layers, batch_size, self.hidden_rnn)).float()\n",
    "        return init\n",
    "    \n",
    "net = Net(time_steps, hidden_rnn)\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "print(x_train.shape)\n",
    "x_lag = get_variable(Variable(torch.from_numpy(x_train_lag[:batch_size,:]).float()))\n",
    "print(x_lag.size())\n",
    "h0 = net.init_hidden(batch_size)\n",
    "c0 = net.init_cell(batch_size)\n",
    "output = net(x_lag, c0, h0)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def standard_normal_pdf(x):\n",
    "    \n",
    "    return (1/math.sqrt(2 * math.pi))*torch.exp(-x**2/2)\n",
    "    \n",
    "\n",
    "def standard_normal_cdf(x):\n",
    "    return 0.5 * (1 + torch.erf(x / math.sqrt(2)))\n",
    "\n",
    "\n",
    "def censored_NLL(output, labels, supply):\n",
    "    x = torch.squeeze(output)\n",
    "    di = get_numpy((labels == supply).float())\n",
    "    loss = get_variable(Variable(torch.zeros(1)))\n",
    "    for i in range(len(output)):\n",
    "        if di[i] == 0:\n",
    "            if get_numpy(standard_normal_pdf(labels[i] - x[i])) == 0:\n",
    "                loss += 20\n",
    "            else:\n",
    "                loss += -torch.log(standard_normal_pdf(labels[i]-x[i]))\n",
    "        if di[i] == 1:\n",
    "            if get_numpy(standard_normal_cdf(labels[i] - x[i])) == 1:\n",
    "                loss += 20\n",
    "            else:\n",
    "                loss += -torch.log(1-standard_normal_cdf(labels[i] - x[i]))\n",
    "             \n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def censored_NLL(output, labels, supply):\n",
    "    x = torch.squeeze(output)\n",
    "    di = get_numpy((labels == supply).float())\n",
    "    loss = get_variable(Variable(torch.zeros(1)))\n",
    "    for i in range(len(output)):\n",
    "        if di[i] == 0:\n",
    "            loss += -torch.log(standard_normal_pdf((labels[i]-x[i])/torch.std(labels)))\n",
    "        if di[i] == 1:\n",
    "            loss += -torch.log(1-standard_normal_cdf((labels[i] - x[i])/torch.std(labels)))\n",
    "             \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Good with MSE loss -> #\n",
    "l_rate = 0.01; optimizer = torch.optim.Adam(net.parameters(), lr = l_rate, weight_decay = 0.01)\n",
    "l_rate = 0.001; optimizer = torch.optim.Adam(net.parameters(), lr = l_rate, weight_decay = 0.01) #R^2 ~ 0.34\n",
    "\n",
    "#l_rate = 0.005; optimizer = torch.optim.Adam(net.parameters(), lr = l_rate, weight_decay = 0.1)\n",
    "#l_rate = 0.0001; optimizer = optim.SGD(net.parameters(), lr=l_rate, weight_decay = 0.1) #R^2 ~ 0.357\n",
    "#l_rate = 0.0001; optimizer = torch.optim.RMSprop(net.parameters(), lr = l_rate, weight_decay = 0.01)\n",
    "\n",
    "#criterion = censored_NLL\n",
    "criterion = nn.MSELoss()\n",
    "#epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm\n",
    "\n",
    "\n",
    "num_samples_train = x_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = x_valid.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "losses = []\n",
    "losses_valid = []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "h0 = get_variable(net.init_hidden(batch_size))\n",
    "c0 = get_variable(net.init_cell(batch_size))\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "\n",
    "train_losses = []; valid_losses = [];\n",
    "train_acc, valid_acc = [], []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch +=1\n",
    "    \n",
    "    cur_loss = 0\n",
    "    net.train()\n",
    "    for i in range(num_batches_train):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        x_batch = get_variable(Variable(torch.from_numpy(x_train[slce])).float())\n",
    "        x_lag_batch = get_variable(Variable(torch.from_numpy(x_train_lag[slce])))\n",
    "        supply_batch = get_variable(Variable(torch.from_numpy(supply_train[slce])).float())\n",
    "        target_batch = get_variable(Variable(torch.from_numpy(targets_train[slce]).float()))\n",
    "        \n",
    "        output = net(x_lag_batch, h0, c0)\n",
    "        \n",
    "        # compute gradients given loss\n",
    "        target_batch = get_variable(Variable(torch.from_numpy(targets_train[slce]).float()))\n",
    "       \n",
    "        \n",
    "        #batch_loss = criterion(output, target_batch, supply_batch)\n",
    "        batch_loss = criterion(output, target_batch) # MSE Loss \n",
    "        \n",
    "                \n",
    "        #print(batch_loss)\n",
    "        if np.isinf(get_numpy(batch_loss)):\n",
    "            print(\"Error - loss is inf!\")\n",
    "            sys.exit() \n",
    "        if np.isnan(get_numpy(batch_loss)):\n",
    "            print(\"Error - loss is Nan!\")\n",
    "            sys.exit() \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        #clip_grad_norm(net.parameters(), 10) #0.25 used in example\n",
    "           \n",
    "        \n",
    "        optimizer.step()\n",
    "        cur_loss += batch_loss\n",
    "    losses.append(get_numpy(cur_loss / batch_size))\n",
    "    \n",
    "    \n",
    "    \n",
    "    net.eval()\n",
    "    ### Evaluate training\n",
    "    train_preds, train_targs = [], []\n",
    "    for i in range(num_batches_train):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        x_batch = get_variable(Variable(torch.from_numpy(x_train[slce])).float())\n",
    "        x_lag_batch = get_variable(Variable(torch.from_numpy(x_train_lag[slce])))\n",
    "        output = net.forward(x_lag_batch, h0, c0);\n",
    "        preds = get_numpy(output)\n",
    "        \n",
    "        train_targs += list(targets_train[slce])\n",
    "        train_preds += list(preds)\n",
    "    \n",
    "    ### Evaluate validation\n",
    "    val_loss = 0\n",
    "    val_preds, val_targs = [], []\n",
    "    for i in range(num_batches_valid):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        x_val = get_variable(Variable(torch.from_numpy(x_valid[slce]).float()))\n",
    "        x_lag_val = get_variable(Variable(torch.from_numpy(x_valid_lag[slce]).float()))\n",
    "        supply_val = get_variable(Variable(torch.from_numpy(supply_valid[slce])).float())\n",
    "        target_val = get_variable(Variable(torch.from_numpy(targets_valid[slce]).float()))\n",
    "        \n",
    "        output_valid = net.forward(x_lag_val, h0, c0)\n",
    "        #valid_loss = criterion(output_valid, target_val, supply_val)\n",
    "        valid_loss = criterion(output_valid, target_val)\n",
    "        \n",
    "        preds = get_numpy(output_valid)\n",
    "        \n",
    "        val_preds += list(preds)\n",
    "        val_targs += list(targets_valid[slce])\n",
    "        \n",
    "        val_loss += valid_loss   \n",
    "    losses_valid.append(get_numpy(val_loss / batch_size))\n",
    "\n",
    "    train_acc_cur = r2_score(train_targs, train_preds)\n",
    "    valid_acc_cur = r2_score(val_targs, val_preds)\n",
    "    \n",
    "    train_acc.append(train_acc_cur)\n",
    "    valid_acc.append(valid_acc_cur)\n",
    "    \n",
    "    \n",
    "    #predicted_valid = get_numpy(model.forward(inputs_val, h0, c0))\n",
    "    #predicted_valid = predicted_valid[:,0,0]\n",
    "    #valid_acc.append(r2_score(targets_val, predicted_val))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('epoch {}, loss {}, train accuracy {}, valid accuracy {}'.format(epoch,losses[epoch-1], train_acc[epoch-1], valid_acc[epoch-1]))\n",
    "    #print('epoch {}, loss {}, train accuracy {}'.format(epoch,loss.data[0], train_acc[epoch-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
